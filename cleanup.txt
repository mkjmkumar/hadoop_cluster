docker container list

docker kill 30bce9263026  0c7f7a87f117  3b0dadadf5d0

docker container list -a

docker container rm namenode

docker container rm datanode1

docker container rm datanode2

docker network rm hadoop

docker network create --driver bridge hadoop

docker create -it -p 8088:8088 -p 50070:50070 -p 50075:50075 -p 2122:2122  --net hadoop --name namenode --hostname namenode --memory 1024m --cpus 2 mukeshkumar/hadoop_cluster


Create and start datanode containers with the Docker image you have just built or pulled (upto 8 datanodes currently possible, to add more edit "/usr/local/hadoop/etc/hadoop/slaves" and restart the cluster)


docker run -itd --name datanode1 --net hadoop --hostname datanode1 --memory 1024m --cpus 2 mukeshkumar/hadoop_cluster

docker run -itd --name datanode2 --net hadoop --hostname datanode2 --memory 1024m --cpus 2 mukeshkumar/hadoop_cluster

docker start namenode

If building image from docker file before starting cluster just update the /usr/local/hadoop/etc/hadoop/slaves with IP addresses.
docker exec -it namenode bash
vi /usr/local/hadoop/etc/hadoop/slaves
Enter IP of datanode1 and datanode2

docker exec -it namenode //etc//bootstrap.sh start_cluster

Test to go at bash of node - docker exec -it namenode bash


Resource Manager UI at

http://localhost:8088

You should be able to access the HDFS UI at

http://localhost:50070

Solved : Protocol tcp Port Exclusion Ranges issues when running Hadoop on Docker

When we run Docker community edition-CE (https://docs.docker.com/docker-for-windows/install/) on Microsoft Windows, under system requriement it clearly says "Hyper-V and Containers Windows features must be enabled." 
to Docker on Windows. But in case you are using Docker EE(Engine â€“ Enterprise) you might not requried the Hyper-V enabled. I think we developers might not going to use docker EE just because Hyper-V reserve some ports those are required by Hadoop default configurations. 

So as of now you got the idea that Hadoop uses certain ports to communicated with datanode and expose http URI for hdfs.

Now lets say my Hyper-V enabled and it reserve some ports to switch communication between linux and windows systems. 
netsh interface ipv4 show excludedportrange protocol=tcp
     49848       49947      
     50000       50059 
     50160       50259      
     50260       50359      
     50360       50459      
     50460       50559      
     50780       50879      
     55609       55708 

Now as per the documentation from Hadoop below is the snippet to showcase the default ports used by Hadoop:-
More you can find at https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2_1.html

Daemon                   Default Port  Configuration Parameter
-----------------------  ------------ ----------------------------------
Namenode                 50070        dfs.http.address
Datanodes                50075        dfs.datanode.http.address
Secondarynamenode        50090        dfs.secondary.http.address
Backup/Checkpoint node?  50105        dfs.backup.http.address
Jobracker                50030        mapred.job.tracker.http.address
Tasktrackers             50060        mapred.task.tracker.http.address

So from above two it clearly understood that port conflic would happen if we run Hadoop container on docker. I had this issue and get resolved by following the below steps:-
We have two options one that I can change those default ports of Hadoop in conflict with Hyper-V or I can just go ahead modify this exclusion list. So lator is the workaround worked for me to save time, the steps are: 
1. Disable hyper-v (which will required a couple of restarts)
dism.exe /Online /Disable-Feature:Microsoft-Hyper-V

2. When you finish all the required restarts, reserve the port you want so hyper-v doesn't reserve it back
netsh int ipv4 add excludedportrange protocol=tcp startport=50070 numberofports=1
netsh int ipv4 add excludedportrange protocol=tcp startport=50075 numberofports=1
netsh int ipv4 add excludedportrange protocol=tcp startport=50090 numberofports=1

3. Re-Enable hyper-V (which will require a couple of restart)
dism.exe /Online /Enable-Feature:Microsoft-Hyper-V /All

when your system is back, you will be able to bind to that port successfully and the new exclision list looks like below in my system:-
PS C:\Users\mukesh.kumar> netsh interface ipv4 show excludedportrange protocol=tcp

Protocol tcp Port Exclusion Ranges

Start Port    End Port
----------    --------
     49694       49793
     49794       49893
     50010       50010     *
     50020       50020     *
     50030       50030     *
     50060       50060     *
     50070       50070     *
     50075       50075     *
     50090       50090     *
     50100       50100     *
     50105       50105     *
     50106       50205
     50230       50329
     50624       50723
     61059       61158

* - Administered port exclusions.

Now, it seems my Hadoop Cluster is working find and If you want the Hadoop cluster docker installation step please visit to my github repo https://github.com/mkjmkumar/hadoop_cluster/blob/master/README.md

You can find the Hadoop image published to docker hub for direct download(In case you dont want the image to be build using docker file) 